import yaml
import sys
import pathlib
import Bio.SeqIO.QualityIO as QualityIO
import gzip
import collections
import subprocess
import re
import os

config_file = sys.argv[1]
threads = int(sys.argv[2])
SCRIPTFOLDER = os.path.dirname(os.path.realpath(__file__)) + "/"


def count_filtered_pairs(file):
    """
    Extracts the count of paired reads written after filtering from a log file generated by cutadapt.

    Parameters:
    - file (str): Path to the log file generated by cutadapt.

    Returns:
    - int: The count of paired reads written after filtering.
    """
    forward = file.split('/')[-1].replace('cutadapt_', '').replace('.log', '').split('---')[
        0]  # cutadapt_515r_parada___v4---926r___v5.log
    reverse = file.split('/')[-1].replace('cutadapt_', '').replace('.log', '').split('---')[1]
    forward_name = forward.split('___')[0]
    forward_region = forward.split('___')[1]
    reverse_name = reverse.split('___')[0]
    reverse_region = reverse.split('___')[1]

    with open(file) as handle:
        for line in handle:
            if 'Pairs written (passing filters):' in line:
                return int(line.split()[-2].replace(',', ''))


def get_sequences(count, file):
    """
    Reads and retrieves a specified number of sequences (in FASTQ format) from a gzipped file.

    Parameters:
    - count (int): The number of sequences to retrieve from the file.
    - file (str): Path to the gzipped FASTQ file.

    Returns:
    - list: A list containing tuples of (header, sequence, quality) for each retrieved sequence.
    """
    data = []
    with gzip.open(file, 'rt') as handle:
        for cnt, (header, sequence, qual) in enumerate(QualityIO.FastqGeneralIterator(handle), 1):
            if cnt <= count:
                data.append((header, sequence, qual))
            else:
                break
    return data


def where_are_the_Ns(file, required_read_length):
    """
    Determines the positions and frequency of 'N' nucleotides in sequences from a FASTQ file.

    Parameters:
    - file (str): Path to the FASTQ file.
    - required_read_length (int): Desired length of the read sequences to analyze.

    Returns:
    - tuple: A tuple containing three elements:
             - total_N (int): Total count of 'N' nucleotides found in the sequences.
             - sequences_with_N (int): Number of sequences containing at least one 'N'.
             - positions (collections.Counter): A Counter object storing positions of 'N' occurrences
                                                in the analyzed sequences and their frequencies.
    """
    positions = collections.Counter()
    sequences_with_N = 0
    with open(file) as handle:
        for cnt, (header, sequence, qual) in enumerate(QualityIO.FastqGeneralIterator(handle), 1):
            sequence = sequence[:required_read_length]
            if 'N' in sequence:
                sequences_with_N += 1
                for position in [_.start() for _ in re.finditer('N', sequence)]:
                    positions[position] += 1
    total_N = sum(list(positions.values()))
    return total_N, sequences_with_N, positions


def get_read_length(file):
    """
    Retrieve the lengths of sequences from a FastQ file.

    Parameters:
    - file (str): The path to the FastQ file to be processed.

    Returns:
    - reads (list): A list containing the lengths of all sequences present in the FastQ file.
    """
    reads = []
    with open(file) as handle:
        for cnt, (header, sequence, qual) in enumerate(QualityIO.FastqGeneralIterator(handle), 1):
            reads.append(len(sequence))
    return reads


def determine_reads_inserts_and_bases(f1, f2):
    """
    Determine the number of reads, inserts, and total bases from paired FastQ files.

    Parameters:
    - f1 (str): Path to the first paired-end FastQ file.
    - f2 (str): Path to the second paired-end FastQ file.

    Returns:
    - tuple: A tuple containing three elements:
        - reads (int): Total number of reads (number of reads in first file duplicated).
        - inserts (int): Total number of inserts (number of reads in the first file).
        - bases (int): Total number of bases from both files.
    """
    r1_reads = []
    r2_reads = []
    with open(f1) as handle:
        for cnt, (header, sequence, qual) in enumerate(QualityIO.FastqGeneralIterator(handle), 1):
            r1_reads.append(len(sequence))
    with open(f2) as handle:
        for cnt, (header, sequence, qual) in enumerate(QualityIO.FastqGeneralIterator(handle), 1):
            r2_reads.append(len(sequence))
    reads = len(r1_reads) * 2
    inserts = len(r1_reads)
    bases = sum(r1_reads) + sum(r2_reads)
    return reads, inserts, bases


# Process configuration data from YAML file to extract necessary parameters.
yaml_data = {}
with open(config_file) as handle:
    yaml_data = yaml.safe_load(handle)

data_dir = yaml_data['data_dir']
samples = yaml_data['sample_file']
blocklist = yaml_data['blocklist']
primer_file = yaml_data['primers']

if yaml_data.get('FORWARD_PRIMER_SEQUENCE') is not None:
    primer_pairs = [([yaml_data['FORWARD_PRIMER_SEQUENCE'], yaml_data['FORWARD_PRIMER_NAME'], 'vn'],
                     [yaml_data['REVERSE_PRIMER_SEQUENCE'], yaml_data['REVERSE_PRIMER_NAME'], 'vn'])]
else:
    with open(primer_file) as handle:
        primers = {}
        primer_pairs = []
        for line in handle:
            splits = line.strip().split()
            if line.startswith('#'):
                forward_primer = primers[splits[1]]
                reverse_primer = primers[splits[-1]]
                primer_pairs.append((forward_primer, reverse_primer))
            else:
                primers[splits[2]] = splits[1:]

# Process sample and blocklist information to filter valid sample names.
SAMPLENAMES = set()
BLOCKLISTNAMES = set()
with open(blocklist) as handle:
    for line in handle:
        BLOCKLISTNAMES.add(line.strip())

with open(samples) as handle:
    for line in handle:
        samplename = line.strip()
        if samplename not in BLOCKLISTNAMES:
            SAMPLENAMES.add(samplename)

# Create folder and file paths
temp_folder = data_dir + '/temp_folder/'
pathlib.Path(temp_folder).mkdir(parents=True, exist_ok=True)
raw_r1_file = temp_folder + 'raw_R1.fastq'
raw_r2_file = temp_folder + 'raw_R2.fastq'

# Process and subset read files for analysis.
total_reads = 0
print('Subsetting read files.')

dest_r1_file = open(raw_r1_file, 'w')
dest_r2_file = open(raw_r2_file, 'w')

for sample in SAMPLENAMES:
    source_r1_file = data_dir + '/0raw/' + sample + '/' + sample + '_R1.fastq.gz'
    source_r2_file = data_dir + '/0raw/' + sample + '/' + sample + '_R2.fastq.gz'
    maxcount = 5000
    r1_reads = get_sequences(maxcount, source_r1_file)
    r2_reads = get_sequences(maxcount, source_r2_file)
    for (header, sequence, qual) in r1_reads:
        dest_r1_file.write(f'@{header}\n{sequence}\n+\n{qual}\n')
        total_reads += 1
    for (header, sequence, qual) in r2_reads:
        dest_r2_file.write(f'@{header}\n{sequence}\n+\n{qual}\n')

dest_r1_file.close()
dest_r2_file.close()

print('FORWARD_PRIMER\tFORWARD_PRIMER_NAME\tREVERSE_PRIMER\tREVERSE_PRIMER_NAME\tRAW_INSERTS\tAVERAGE_READ_LENGTH_R1'
      '\tAVERAGE_READ_LENGTH_R2\tCUTADAPT_INSERTS\tPRIMER_HITS\tR1_READS_W_N\tR2_READS_W_N\tPARAM=TRUNCLENR1\tPARAM'
      '=TRUNCLENR2\tPARAM=QCMINLEN\tPARAM=MAXEE\tQC_INSERTS\tPERC_INSERTS\tPLANNED_OVERLAP\tESTIMATED_INSERT_SIZE')

raw_reads, raw_inserts, raw_bases = determine_reads_inserts_and_bases(raw_r1_file, raw_r2_file)
raw_readlength_r1 = get_read_length(raw_r1_file)
raw_readlength_r2 = get_read_length(raw_r2_file)
raw_readlength_r1_avg = int(sum(raw_readlength_r1) / len(raw_readlength_r1))
raw_readlength_r2_avg = int(sum(raw_readlength_r2) / len(raw_readlength_r2))

# Process primer pairs to perform read filtering and analysis
for forward_primer, reverse_primer in primer_pairs:
    # Define file paths
    cutadapt_r1_file = temp_folder + f'cutadapt_{forward_primer[1]}___{forward_primer[2]}---{reverse_primer[1]}___{reverse_primer[2]}_R1.fastq'
    cutadapt_r2_file = temp_folder + f'cutadapt_{forward_primer[1]}___{forward_primer[2]}---{reverse_primer[1]}___{reverse_primer[2]}_R2.fastq'
    cutadapt_log_file = temp_folder + f'cutadapt_{forward_primer[1]}___{forward_primer[2]}---{reverse_primer[1]}___{reverse_primer[2]}.log'

    # Run cutadapt
    allowUntrimmed = "--discard-untrimmed" if not yaml_data['allowUntrimmed'] else ""
    cutadapt_command_stub = f'cutadapt -O 12 {allowUntrimmed} -g {forward_primer[0]} -G {reverse_primer[0]} -o {cutadapt_r1_file} -p {cutadapt_r2_file} {raw_r1_file} {raw_r2_file} -j {threads} --pair-adapters --minimum-length 75 &> {cutadapt_log_file}'
    subprocess.check_call(cutadapt_command_stub, shell=True)

    # Process cutadapt output
    pairs_written = count_filtered_pairs(cutadapt_log_file)
    perc_kept = int(100.0 * pairs_written / total_reads)
    cutadapt_reads, cutadapt_inserts, cutadapt_bases = determine_reads_inserts_and_bases(cutadapt_r1_file,
                                                                                         cutadapt_r2_file)
    if perc_kept < 10.0:
        print(f'{forward_primer[0]}\t{forward_primer[1]}\t{reverse_primer[0]}\t{reverse_primer[1]}\t{raw_inserts}\t{raw_readlength_r1_avg}\t{raw_readlength_r2_avg}\t{cutadapt_inserts}\t{perc_kept}\tNA\tNA\tNA\tNA\tNA\tNA\tNA\tNA\tNA\tNA')
        continue

    else:
        # Define variables
        planned_overlap = 45
        maxees = [0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
        readlength_r1 = get_read_length(cutadapt_r1_file)
        readlength_r2 = get_read_length(cutadapt_r2_file)

        # Calculations
        readlength_r1_avg = int(sum(readlength_r1) / len(readlength_r1))
        readlength_r2_avg = int(sum(readlength_r2) / len(readlength_r2))
        estimated_insert_size = abs(
            int(re.findall(r'\d+', reverse_primer[1])[0]) - int(re.findall(r'\d+', forward_primer[1])[0])) - len(
            forward_primer[0]) - len(reverse_primer[0]) + 1
        required_read_length = int((2 * planned_overlap + estimated_insert_size) / 2)
        r1_required_read_length = required_read_length + 20
        r2_required_read_length = required_read_length - 20
        qcminlength = r2_required_read_length - 10
        r2_total_N, r2_sequences_with_N, r2_N_positions = where_are_the_Ns(cutadapt_r2_file, r2_required_read_length)
        r1_total_N, r1_sequences_with_N, r1_N_positions = where_are_the_Ns(cutadapt_r1_file, r1_required_read_length)

        # Run filterandtrim for each maxee (maximum expected errors)
        for maxee in maxees:
            r1_out_file = cutadapt_r1_file.replace('_R1.fastq', '_maxee-' + str(maxee) + '_R1.fastq')
            r2_out_file = cutadapt_r1_file.replace('_R1.fastq', '_maxee-' + str(maxee) + '_R2.fastq')

            command = f'Rscript {SCRIPTFOLDER}dada2_filterandtrim.R {cutadapt_r1_file} {cutadapt_r2_file} {r1_out_file} {r2_out_file} {maxee} 2 0 FALSE {qcminlength} {r1_required_read_length} {r2_required_read_length} {threads} &> {r1_out_file}.log'
            subprocess.check_call(command, shell=True)
            pathlib.Path(r1_out_file).touch(exist_ok=True)
            pathlib.Path(r2_out_file).touch(exist_ok=True)

            # Analyze filterandtrim output and print
            reads, inserts, bases = determine_reads_inserts_and_bases(r1_out_file, r2_out_file)
            perc_qc_inserts = int(100.0 * inserts / cutadapt_inserts)
            print(f'{forward_primer[0]}\t{forward_primer[1]}\t{reverse_primer[0]}\t{reverse_primer[1]}\t{raw_inserts}\t{raw_readlength_r1_avg}\t{raw_readlength_r2_avg}\t{cutadapt_inserts}\t{perc_kept}\t{r1_sequences_with_N}\t{r2_sequences_with_N}\t{r1_required_read_length}\t{r2_required_read_length}\t{qcminlength}\t{maxee}\t{inserts}\t{perc_qc_inserts}\t{planned_overlap}\t{estimated_insert_size}')
